{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b002d1b8-6ad5-4dab-b084-48dce3208e36",
   "metadata": {},
   "source": [
    "# Backpropagation for single neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b010a9a-8a41-4020-bb85-67dd3362b083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration - 1, loss:36.0\n",
      "iteration - 2, loss:33.872399999999985\n",
      "iteration - 3, loss:31.870541159999995\n",
      "iteration - 4, loss:29.98699217744401\n",
      "iteration - 5, loss:28.21476093975706\n",
      "iteration - 6, loss:26.54726856821742\n",
      "iteration - 7, loss:24.978324995835766\n",
      "iteration - 8, loss:23.502105988581878\n",
      "iteration - 9, loss:22.113131524656684\n",
      "iteration - 10, loss:20.80624545154949\n",
      "iteration - 11, loss:19.576596345362915\n",
      "iteration - 12, loss:18.419619501351963\n",
      "iteration - 13, loss:17.331019988822064\n",
      "iteration - 14, loss:16.306756707482677\n",
      "iteration - 15, loss:15.343027386070442\n",
      "iteration - 16, loss:14.43625446755368\n",
      "iteration - 17, loss:13.583071828521266\n",
      "iteration - 18, loss:12.780312283455652\n",
      "iteration - 19, loss:12.024995827503426\n",
      "iteration - 20, loss:11.314318574097976\n",
      "iteration - 21, loss:10.645642346368787\n",
      "iteration - 22, loss:10.016484883698395\n",
      "iteration - 23, loss:9.424510627071816\n",
      "iteration - 24, loss:8.867522049011871\n",
      "iteration - 25, loss:8.34345149591527\n",
      "iteration - 26, loss:7.850353512506679\n",
      "iteration - 27, loss:7.386397619917536\n",
      "iteration - 28, loss:6.949861520580408\n",
      "iteration - 29, loss:6.539124704714106\n",
      "iteration - 30, loss:6.152662434665503\n",
      "iteration - 31, loss:5.7890400847767705\n",
      "iteration - 32, loss:5.446907815766464\n",
      "iteration - 33, loss:5.124995563854671\n",
      "iteration - 34, loss:4.8221083260308575\n",
      "iteration - 35, loss:4.537121723962434\n",
      "iteration - 36, loss:4.268977830076255\n",
      "iteration - 37, loss:4.016681240318748\n",
      "iteration - 38, loss:3.7792953790159096\n",
      "iteration - 39, loss:3.5559390221160707\n",
      "iteration - 40, loss:3.345783025909011\n",
      "iteration - 41, loss:3.148047249077789\n",
      "iteration - 42, loss:2.9619976566572896\n",
      "iteration - 43, loss:2.786943595148845\n",
      "iteration - 44, loss:2.622235228675549\n",
      "iteration - 45, loss:2.4672611266608238\n",
      "iteration - 46, loss:2.321445994075166\n",
      "iteration - 47, loss:2.1842485358253256\n",
      "iteration - 48, loss:2.0551594473580463\n",
      "iteration - 49, loss:1.9336995240191863\n",
      "iteration - 50, loss:1.8194178821496518\n",
      "iteration - 51, loss:1.7118902853146067\n",
      "iteration - 52, loss:1.6107175694525138\n",
      "iteration - 53, loss:1.515524161097869\n",
      "iteration - 54, loss:1.4259566831769857\n",
      "iteration - 55, loss:1.3416826432012259\n",
      "iteration - 56, loss:1.2623891989880334\n",
      "iteration - 57, loss:1.18778199732784\n",
      "iteration - 58, loss:1.1175840812857634\n",
      "iteration - 59, loss:1.0515348620817766\n",
      "iteration - 60, loss:0.9893891517327431\n",
      "iteration - 61, loss:0.930916252865338\n",
      "iteration - 62, loss:0.8758991023209968\n",
      "iteration - 63, loss:0.8241334653738256\n",
      "iteration - 64, loss:0.7754271775702323\n",
      "iteration - 65, loss:0.7295994313758316\n",
      "iteration - 66, loss:0.6864801049815187\n",
      "iteration - 67, loss:0.6459091307771115\n",
      "iteration - 68, loss:0.6077359011481847\n",
      "iteration - 69, loss:0.571818709390327\n",
      "iteration - 70, loss:0.5380242236653578\n",
      "iteration - 71, loss:0.5062269920467349\n",
      "iteration - 72, loss:0.47630897681677353\n",
      "iteration - 73, loss:0.4481591162869011\n",
      "iteration - 74, loss:0.4216729125143454\n",
      "iteration - 75, loss:0.3967520433847474\n",
      "iteration - 76, loss:0.3733039976207088\n",
      "iteration - 77, loss:0.35124173136132436\n",
      "iteration - 78, loss:0.3304833450378702\n",
      "iteration - 79, loss:0.3109517793461322\n",
      "iteration - 80, loss:0.29257452918677535\n",
      "iteration - 81, loss:0.27528337451183676\n",
      "iteration - 82, loss:0.25901412707818716\n",
      "iteration - 83, loss:0.24370639216786655\n",
      "iteration - 84, loss:0.22930334439074554\n",
      "iteration - 85, loss:0.21575151673725296\n",
      "iteration - 86, loss:0.2030006020980815\n",
      "iteration - 87, loss:0.1910032665140846\n",
      "iteration - 88, loss:0.17971497346310225\n",
      "iteration - 89, loss:0.16909381853143338\n",
      "iteration - 90, loss:0.1591003738562249\n",
      "iteration - 91, loss:0.14969754176132236\n",
      "iteration - 92, loss:0.14085041704322837\n",
      "iteration - 93, loss:0.13252615739597357\n",
      "iteration - 94, loss:0.12469386149387143\n",
      "iteration - 95, loss:0.11732445427958357\n",
      "iteration - 96, loss:0.1103905790316602\n",
      "iteration - 97, loss:0.1038664958108892\n",
      "iteration - 98, loss:0.09772798590846558\n",
      "iteration - 99, loss:0.09195226194127534\n",
      "iteration - 100, loss:0.08651788326054576\n",
      "iteration - 101, loss:0.08140467635984766\n",
      "iteration - 102, loss:0.07659365998698062\n",
      "iteration - 103, loss:0.07206697468175022\n",
      "iteration - 104, loss:0.06780781647805834\n",
      "iteration - 105, loss:0.06380037452420508\n",
      "iteration - 106, loss:0.06002977238982451\n",
      "iteration - 107, loss:0.056482012841585764\n",
      "iteration - 108, loss:0.05314392588264784\n",
      "iteration - 109, loss:0.050003119862983315\n",
      "iteration - 110, loss:0.04704793547908108\n",
      "iteration - 111, loss:0.044267402492267266\n",
      "iteration - 112, loss:0.04165119900497404\n",
      "iteration - 113, loss:0.03918961314378035\n",
      "iteration - 114, loss:0.036873507006982977\n",
      "iteration - 115, loss:0.034694282742870286\n",
      "iteration - 116, loss:0.032643850632766785\n",
      "iteration - 117, loss:0.030714599060370322\n",
      "iteration - 118, loss:0.028899366255902493\n",
      "iteration - 119, loss:0.027191413710178584\n",
      "iteration - 120, loss:0.025584401159906914\n",
      "iteration - 121, loss:0.024072363051356495\n",
      "iteration - 122, loss:0.02264968639502138\n",
      "iteration - 123, loss:0.021311089929075627\n",
      "iteration - 124, loss:0.02005160451426725\n",
      "iteration - 125, loss:0.018866554687474075\n",
      "iteration - 126, loss:0.017751541305444478\n",
      "iteration - 127, loss:0.016702425214292563\n",
      "iteration - 128, loss:0.015715311884128023\n",
      "iteration - 129, loss:0.014786536951776086\n",
      "iteration - 130, loss:0.013912652617925996\n",
      "iteration - 131, loss:0.013090414848206543\n",
      "iteration - 132, loss:0.012316771330677616\n",
      "iteration - 133, loss:0.011588850145034585\n",
      "iteration - 134, loss:0.010903949101463065\n",
      "iteration - 135, loss:0.010259525709566468\n",
      "iteration - 136, loss:0.00965318774013127\n",
      "iteration - 137, loss:0.009082684344689475\n",
      "iteration - 138, loss:0.008545897699918217\n",
      "iteration - 139, loss:0.008040835145853157\n",
      "iteration - 140, loss:0.007565621788733219\n",
      "iteration - 141, loss:0.0071184935410191314\n",
      "iteration - 142, loss:0.0066977905727448606\n",
      "iteration - 143, loss:0.0063019511498957235\n",
      "iteration - 144, loss:0.0059295058369368625\n",
      "iteration - 145, loss:0.005579072041973911\n",
      "iteration - 146, loss:0.005249348884293189\n",
      "iteration - 147, loss:0.004939112365231465\n",
      "iteration - 148, loss:0.004647210824446307\n",
      "iteration - 149, loss:0.004372560664721486\n",
      "iteration - 150, loss:0.004114142329436494\n",
      "iteration - 151, loss:0.003870996517766834\n",
      "iteration - 152, loss:0.0036422206235667827\n",
      "iteration - 153, loss:0.003426965384714017\n",
      "iteration - 154, loss:0.0032244317304774505\n",
      "iteration - 155, loss:0.0030338678152062068\n",
      "iteration - 156, loss:0.0028545662273275238\n",
      "iteration - 157, loss:0.002685861363292443\n",
      "iteration - 158, loss:0.002527126956721865\n",
      "iteration - 159, loss:0.0023777737535795864\n",
      "iteration - 160, loss:0.00223724732474303\n",
      "iteration - 161, loss:0.0021050260078507234\n",
      "iteration - 162, loss:0.0019806189707867374\n",
      "iteration - 163, loss:0.0018635643896132343\n",
      "iteration - 164, loss:0.0017534277341871227\n",
      "iteration - 165, loss:0.001649800155096641\n",
      "iteration - 166, loss:0.0015522969659304577\n",
      "iteration - 167, loss:0.001460556215243966\n",
      "iteration - 168, loss:0.0013742373429230384\n",
      "iteration - 169, loss:0.0012930199159562866\n",
      "iteration - 170, loss:0.0012166024389232565\n",
      "iteration - 171, loss:0.0011447012347829176\n",
      "iteration - 172, loss:0.0010770493918072417\n",
      "iteration - 173, loss:0.0010133957727514104\n",
      "iteration - 174, loss:0.0009535040825818078\n",
      "iteration - 175, loss:0.0008971519913012032\n",
      "iteration - 176, loss:0.0008441303086153036\n",
      "iteration - 177, loss:0.0007942422073761319\n",
      "iteration - 178, loss:0.0007473024929201971\n",
      "iteration - 179, loss:0.0007031369155886336\n",
      "iteration - 180, loss:0.0006615815238773228\n",
      "iteration - 181, loss:0.0006224820558161947\n",
      "iteration - 182, loss:0.0005856933663174669\n",
      "iteration - 183, loss:0.0005510788883681015\n",
      "iteration - 184, loss:0.0005185101260655451\n",
      "iteration - 185, loss:0.00048786617761505856\n",
      "iteration - 186, loss:0.00045903328651801555\n",
      "iteration - 187, loss:0.0004319044192847635\n",
      "iteration - 188, loss:0.0004063788681050637\n",
      "iteration - 189, loss:0.0003823618770000461\n",
      "iteration - 190, loss:0.0003597642900693548\n",
      "iteration - 191, loss:0.0003385022205262612\n",
      "iteration - 192, loss:0.00031849673929316324\n",
      "iteration - 193, loss:0.0002996735820009465\n",
      "iteration - 194, loss:0.0002819628733046985\n",
      "iteration - 195, loss:0.0002652988674923804\n",
      "iteration - 196, loss:0.0002496197044235683\n",
      "iteration - 197, loss:0.00023486717989212869\n",
      "iteration - 198, loss:0.00022098652956051694\n",
      "iteration - 199, loss:0.0002079262256634926\n",
      "iteration - 200, loss:0.00019563778572677352\n",
      "final biases - 0.6009044964039992\n",
      "final weights - [-3.3990955  -0.20180899  0.80271349]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "weights = np.array([-3.0, -1.0, 2.0])\n",
    "bias = 1.0\n",
    "inputs = np.array([1.0, -2.0, 3.0])\n",
    "target=0\n",
    "control=0.001\n",
    "def relu(inp):\n",
    "    return np.maximum(inp,0)\n",
    "def relu_diff(inp):\n",
    "    return np.where(inp>0,1,0)\n",
    "for i in range(200):\n",
    "    sample=np.dot(weights,inputs)+bias\n",
    "    relu_=relu(sample)\n",
    "    loss=(target-relu_)**2\n",
    "\n",
    "    dloss=2*relu_\n",
    "    drelu=relu_diff(sample)\n",
    "    dsum=1\n",
    "    dbias=1\n",
    "    dlin=inputs\n",
    "\n",
    "    dfinal_loss=dloss*drelu*dsum*dlin\n",
    "    dfinal_bias=dloss*drelu*dsum*dbias\n",
    "\n",
    "    bias-=control*dfinal_bias\n",
    "    weights-=control*dfinal_loss\n",
    "\n",
    "    print(f\"iteration - {i+1}, loss:{loss}\")\n",
    "\n",
    "print(f\"final biases - {bias}\")\n",
    "print(f\"final weights - {weights}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300964e8-8837-4e82-b88e-ad635917d03c",
   "metadata": {},
   "source": [
    "THUS WE HAVE MINIMIZED THE LOSS WITH EVERY ITEREATION AND HAVE REACHED NEAR TO THE TARGET VALUE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe1e760-a17f-475c-9960-becf1a2200eb",
   "metadata": {},
   "source": [
    "# BACKPROPAGATION FOR A LAYER OF NEURONS\n",
    "-> Initial approach was to write each and every derivative for each neuron in the for loop and get result as stated earlier\n",
    "\n",
    "-> New approach we use matrices to handle multiple neurons without coding for every single neuron's weights and baises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c909d29-00f5-420b-928e-62b7842905d3",
   "metadata": {},
   "source": [
    "# _GRADIENTS OF THE LOSS WITH RESPECT TO WEIGHTS,INPUTS AND BIASES_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05c90318-7e25-462f-b922-f055191bc921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5  0.5  0.5]\n",
      " [20.1 20.1 20.1]\n",
      " [10.9 10.9 10.9]\n",
      " [ 4.1  4.1  4.1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Passed-in gradient from the next layer\n",
    "# for the purpose of this example we're going to use\n",
    "# an array of an incremental gradient values\n",
    "dvalues = np.array([[1., 1., 1.],\n",
    " [2., 2., 2.],\n",
    " [3., 3., 3.]])\n",
    "# We have 3 sets of inputs - samples\n",
    "inputs = np.array([[1, 2, 3, 2.5],\n",
    " [2., 5., -1., 2],\n",
    " [-1.5, 2.7, 3.3, -0.8]])\n",
    "# sum weights of given input\n",
    "# and multiply by the passed-in gradient for this neuron\n",
    "dweights = np.dot(inputs.T, dvalues)\n",
    "print(dweights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e99ed95b-c264-43fa-9e2f-999a9bc92e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6. 6. 6.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Passed-in gradient from the next layer\n",
    "# for the purpose of this example we're going to use\n",
    "# an array of an incremental gradient values\n",
    "dvalues = np.array([[1., 1., 1.],\n",
    " [2., 2., 2.],\n",
    " [3., 3., 3.]])\n",
    "# One bias for each neuron\n",
    "# biases are the row vector with a shape (1, neurons)\n",
    "biases = np.array([[2, 3, 0.5]])\n",
    "# dbiases - sum values, do this over samples (first axis), keepdims\n",
    "# since this by default will produce a plain list -\n",
    "dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "print(dbiases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed4a3a85-d1ec-41e5-b9ba-ede3f05fd0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.44 -0.38 -0.07  1.37]\n",
      " [ 0.88 -0.76 -0.14  2.74]\n",
      " [ 1.32 -1.14 -0.21  4.11]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Passed-in gradient from the next layer\n",
    "# for the purpose of this example we're going to use\n",
    "# an array of an incremental gradient values\n",
    "dvalues = np.array([[1., 1., 1.],\n",
    " [2., 2., 2.],\n",
    " [3., 3., 3.]])\n",
    "# We have 3 sets of weights - one set for each neuron\n",
    "# we have 4 inputs, thus 4 weights\n",
    "# recall that we keep weights transposed\n",
    "weights = np.array([[0.2, 0.8, -0.5, 1],\n",
    " [0.5, -0.91, 0.26, -0.5],\n",
    " [-0.26, -0.27, 0.17, 0.87]]).T\n",
    "# sum weights of given input\n",
    "# and multiply by the passed-in gradient for this neuron\n",
    "dinputs = np.dot(dvalues, weights.T)\n",
    "print(dinputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6408d1ab-afd8-4846-b6eb-8af5270c6bc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
